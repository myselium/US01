{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning - Wellbeing in Shanghai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"C:/Users/jeane/Documents/Travail/UTSEUS/Urban Data Hackathon/pickles/meters/data4hapiness_Scaled.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameColumn = []\n",
    "for column in df:\n",
    "    if column != \"clean\" and column != \"smell\" and column != \"noise\" and column != \"hapiness\":\n",
    "        nameColumn.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DirtyNotsmellyNoisy',\n",
       " 'DirtyNotsmellyQuite',\n",
       " 'DirtySmellyQuite',\n",
       " 'CleanSmellyQuite',\n",
       " 'DirtySmellyNoisy',\n",
       " 'CleanSmellyNoisy',\n",
       " 'CleanNotsmellyNoisy',\n",
       " 'CleanNotsmellyQuite']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[nameColumn]\n",
    "Y = df['hapiness']\n",
    "subcat_diff = list(set(df['hapiness']))\n",
    "subcat_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8 different classes :\n",
    "\n",
    "    - CleanNotsmellyNoisy\n",
    "    - CleanNotsmellyQuite\n",
    "    - CleanSmellyNoisy\n",
    "    - CleanSmellyQuite\n",
    "    - DirtyNotsmellyNoisy\n",
    "    - DirtyNotsmellyQuite\n",
    "    - DirtySmellyNoisy\n",
    "    - DirtySmellyQuite\n",
    "    \n",
    "So with a random distribution, we get a score of 12.5.\n",
    "Our goal; having a higher prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3697478991596639"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=40, random_state=0, n_estimators = 100)\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "param_grid = {'max_depth': [17], \"n_estimators\" : [15]}\n",
    "search = GridSearchCV(RandomForestClassifier(random_state=0), param_grid, cv= ShuffleSplit(n_splits=5))\n",
    "search.fit(X,Y)\n",
    "np.mean(prediction == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4111111111111111"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, Y, cv=ShuffleSplit(n_splits=5))\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "CleanNotsmellyNoisy       0.00      0.00      0.00         7\n",
      "CleanNotsmellyQuite       0.42      0.54      0.47        56\n",
      "   CleanSmellyQuite       0.00      0.00      0.00         2\n",
      "DirtyNotsmellyNoisy       0.00      0.00      0.00        14\n",
      "DirtyNotsmellyQuite       0.31      0.40      0.35        35\n",
      "   DirtySmellyQuite       0.00      0.00      0.00         5\n",
      "\n",
      "           accuracy                           0.37       119\n",
      "          macro avg       0.12      0.16      0.14       119\n",
      "       weighted avg       0.29      0.37      0.32       119\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report:\")\n",
    "print(classification_report(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Agressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35294117647058826"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "clf = PassiveAggressiveClassifier()\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "search = GridSearchCV(PassiveAggressiveClassifier(), {}, cv= ShuffleSplit(n_splits=5))\n",
    "search.fit(X,Y)\n",
    "np.mean(prediction == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
       "                            early_stopping=False, fit_intercept=True,\n",
       "                            loss='hinge', max_iter=1000, n_iter_no_change=5,\n",
       "                            n_jobs=None, random_state=None, shuffle=True,\n",
       "                            tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "                            warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3111111111111111"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, Y, cv=ShuffleSplit(n_splits=5))\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "CleanNotsmellyNoisy       0.00      0.00      0.00         7\n",
      "CleanNotsmellyQuite       0.54      0.45      0.49        56\n",
      "   CleanSmellyNoisy       0.00      0.00      0.00         0\n",
      "   CleanSmellyQuite       0.00      0.00      0.00         2\n",
      "DirtyNotsmellyNoisy       0.20      0.21      0.21        14\n",
      "DirtyNotsmellyQuite       0.33      0.40      0.36        35\n",
      "   DirtySmellyNoisy       0.00      0.00      0.00         0\n",
      "   DirtySmellyQuite       0.00      0.00      0.00         5\n",
      "\n",
      "           accuracy                           0.35       119\n",
      "          macro avg       0.13      0.13      0.13       119\n",
      "       weighted avg       0.38      0.35      0.36       119\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report:\")\n",
    "print(classification_report(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Booste Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4789915966386555"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "param_grid = {\"n_estimators\" : [12, 13, 15]}\n",
    "search = GridSearchCV(clf, param_grid, cv= ShuffleSplit(n_splits=5))\n",
    "search.fit(X_train, Y_train)\n",
    "np.mean(prediction == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 13}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3583333333333334"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X_train, Y_train, cv=ShuffleSplit(n_splits=5))\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "CleanNotsmellyNoisy       0.00      0.00      0.00         7\n",
      "CleanNotsmellyQuite       0.47      0.98      0.64        56\n",
      "   CleanSmellyQuite       0.00      0.00      0.00         2\n",
      "DirtyNotsmellyNoisy       0.00      0.00      0.00        14\n",
      "DirtyNotsmellyQuite       0.67      0.06      0.11        35\n",
      "   DirtySmellyQuite       0.00      0.00      0.00         5\n",
      "\n",
      "           accuracy                           0.48       119\n",
      "          macro avg       0.19      0.17      0.12       119\n",
      "       weighted avg       0.42      0.48      0.33       119\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report:\")\n",
    "print(classification_report(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostClassifier(n_estimators=100, random_state=0).fit(X, Y) \n",
    "pickle.dump(model, open(\"C:/Users/jeane/Documents/Travail/UTSEUS/Urban Data Hackathon/pickles/meters/Rendu_final_US01.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35294117647058826"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC(random_state=0)\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "param_grid = {'C': [10000]}\n",
    "search = GridSearchCV(LinearSVC(), param_grid, cv=ShuffleSplit(n_splits=5))\n",
    "search.fit(X_train, Y_train)\n",
    "np.mean(prediction == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3611111111111111"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, Y, cv=ShuffleSplit(n_splits=5))\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "CleanNotsmellyNoisy       0.12      0.14      0.13         7\n",
      "CleanNotsmellyQuite       0.55      0.46      0.50        56\n",
      "   CleanSmellyNoisy       0.00      0.00      0.00         0\n",
      "   CleanSmellyQuite       0.00      0.00      0.00         2\n",
      "DirtyNotsmellyNoisy       0.18      0.14      0.16        14\n",
      "DirtyNotsmellyQuite       0.31      0.37      0.34        35\n",
      "   DirtySmellyQuite       0.00      0.00      0.00         5\n",
      "\n",
      "           accuracy                           0.35       119\n",
      "          macro avg       0.17      0.16      0.16       119\n",
      "       weighted avg       0.38      0.35      0.36       119\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report:\")\n",
    "print(classification_report(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(248.0, 308.0, 'X[131] <= 0.463\\nentropy = 0.664\\nsamples = 239\\nvalue = [20, 102, 2, 1, 22, 89, 1, 2]'),\n",
       " Text(124.0, 184.79999999999998, 'X[28] <= 0.353\\nentropy = 0.673\\nsamples = 221\\nvalue = [20, 87, 2, 1, 21, 87, 1, 2]'),\n",
       " Text(62.0, 61.599999999999966, 'entropy = 0.661\\nsamples = 197\\nvalue = [16, 85, 1, 1, 18, 73, 1, 2]'),\n",
       " Text(186.0, 61.599999999999966, 'entropy = 0.608\\nsamples = 24\\nvalue = [4, 2, 1, 0, 3, 14, 0, 0]'),\n",
       " Text(372.0, 184.79999999999998, 'X[25] <= 0.836\\nentropy = 0.29\\nsamples = 18\\nvalue = [0, 15, 0, 0, 1, 2, 0, 0]'),\n",
       " Text(310.0, 61.599999999999966, 'entropy = 0.117\\nsamples = 16\\nvalue = [0, 15, 0, 0, 1, 0, 0, 0]'),\n",
       " Text(434.0, 61.599999999999966, 'entropy = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 0, 2, 0, 0]')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=2)\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "tree.plot_tree(clf.fit(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3611111111111111"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth=5)\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "param_grid = {'max_depth': [2, 4, 6, 8, 10, 20, 40], 'criterion': ['gini', 'entropy']}\n",
    "search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=ShuffleSplit(n_splits=5))\n",
    "search.fit(X_train, Y_train)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 4}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3833333333333333"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, Y, cv=ShuffleSplit(n_splits=5))\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "CleanNotsmellyNoisy       0.00      0.00      0.00         7\n",
      "CleanNotsmellyQuite       0.38      0.38      0.38        56\n",
      "   CleanSmellyQuite       0.00      0.00      0.00         2\n",
      "DirtyNotsmellyNoisy       0.00      0.00      0.00        14\n",
      "DirtyNotsmellyQuite       0.21      0.37      0.27        35\n",
      "   DirtySmellyQuite       0.00      0.00      0.00         5\n",
      "\n",
      "           accuracy                           0.29       119\n",
      "          macro avg       0.10      0.12      0.11       119\n",
      "       weighted avg       0.24      0.29      0.26       119\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeane\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report:\")\n",
    "print(classification_report(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best Machine learning model is Ada Boost Classifier if we favor the \"accuracy\" score (0.48), and the Linear SVC if we favor the \"weighted avg\" score (0.36)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
